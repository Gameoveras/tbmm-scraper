# ğŸ¤– GitHub Actions ile Otomatik Scraping Kurulumu

Bu dokÃ¼mantasyon, TBMM Kanun Teklifleri Scraper'Ä±n GitHub Actions ile otomatik olarak Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlayan kurulum rehberidir.

## ğŸ“‹ Ã–zellikler

âœ… **Otomatik Zamanlama**: Her gÃ¼n saat 12:00 (TÃ¼rkiye saati) otomatik Ã§alÄ±ÅŸÄ±r  
âœ… **Manuel Tetikleme**: Ä°stediÄŸiniz zaman manuel baÅŸlatabilirsiniz  
âœ… **Otomatik Commit**: Yeni veriler otomatik olarak repository'ye commit edilir  
âœ… **Hata YÃ¶netimi**: Hata durumunda bildirim ve log kayÄ±tlarÄ±  
âœ… **Artifact Yedekleme**: Her Ã§alÄ±ÅŸmanÄ±n sonucu 30 gÃ¼n boyunca saklanÄ±r  
âœ… **Headless Mode**: Sunucu ortamÄ±nda gÃ¶rsel arayÃ¼z olmadan Ã§alÄ±ÅŸÄ±r  

## ğŸš€ Kurulum AdÄ±mlarÄ±

### 1. Repository Ä°zinlerini Ayarla

GitHub repository'nizin Settings bÃ¶lÃ¼mÃ¼nden:

1. **Settings** > **Actions** > **General**
2. **Workflow permissions** bÃ¶lÃ¼mÃ¼nde:
   - âœ… `Read and write permissions` seÃ§in
   - âœ… `Allow GitHub Actions to create and approve pull requests` iÅŸaretleyin
3. **Save** butonuna tÄ±klayÄ±n

### 2. Workflow DosyasÄ±nÄ± Kontrol Et

`.github/workflows/scrape-kanun-teklifleri.yml` dosyasÄ± zaten oluÅŸturulmuÅŸ durumda. Bu dosya:

- Her gÃ¼n saat 09:00 UTC'de Ã§alÄ±ÅŸÄ±r (TÃ¼rkiye saati 12:00)
- Manuel tetiklenebilir
- Kod deÄŸiÅŸikliklerinde test olarak Ã§alÄ±ÅŸÄ±r

### 3. Ä°lk Ã‡alÄ±ÅŸtÄ±rma

#### Otomatik (ZamanlanmÄ±ÅŸ)
Workflow ilk kez yarÄ±n saat 12:00'de otomatik Ã§alÄ±ÅŸacak.

#### Manuel BaÅŸlatma
Hemen test etmek iÃ§in:

1. GitHub repository'nize gidin
2. **Actions** sekmesine tÄ±klayÄ±n
3. Sol menÃ¼den **TBMM Kanun Teklifleri Scraper** workflow'unu seÃ§in
4. **Run workflow** > **Run workflow** butonuna tÄ±klayÄ±n

## âš™ï¸ YapÄ±landÄ±rma

### Ã‡alÄ±ÅŸma ZamanÄ±nÄ± DeÄŸiÅŸtirme

`.github/workflows/scrape-kanun-teklifleri.yml` dosyasÄ±nda cron ifadesini dÃ¼zenleyin:

```yaml
schedule:
  - cron: '0 9 * * *'  # Her gÃ¼n 09:00 UTC (12:00 TR)
```

**Ã–rnekler:**
```yaml
# Her 6 saatte bir
- cron: '0 */6 * * *'

# Her Pazartesi saat 10:00 UTC
- cron: '0 10 * * 1'

# HaftaiÃ§i her gÃ¼n saat 08:00 UTC
- cron: '0 8 * * 1-5'

# Her 12 saatte bir (sabah ve akÅŸam)
- cron: '0 0,12 * * *'
```

Cron ifadelerini oluÅŸturmak iÃ§in: [crontab.guru](https://crontab.guru/)

### Sorgu Parametrelerini DeÄŸiÅŸtirme

`scraper/kanun_teklifleri_scraper.py` dosyasÄ±nda `main()` fonksiyonundaki `fill_search_form()` parametrelerini dÃ¼zenleyin:

```python
# Ã–rnek: Sadece kanunlaÅŸmÄ±ÅŸ teklifler
fill_search_form(
    arama_kelime="",
    donem="Son DÃ¶nem",
    durum="KANUNLAÅTI"
)

# Ã–rnek: Belirli bir kelime
fill_search_form(
    arama_kelime="eÄŸitim",
    donem="Son DÃ¶nem",
    durum=""
)
```

## ğŸ“Š Ã‡alÄ±ÅŸma Durumunu Ä°zleme

### Actions Sekmesi

1. Repository'nize gidin
2. **Actions** sekmesine tÄ±klayÄ±n
3. Son Ã§alÄ±ÅŸtÄ±rmalarÄ± gÃ¶receksiniz:
   - âœ… YeÅŸil: BaÅŸarÄ±lÄ±
   - âŒ KÄ±rmÄ±zÄ±: BaÅŸarÄ±sÄ±z
   - ğŸŸ¡ SarÄ±: Ã‡alÄ±ÅŸÄ±yor

### DetaylÄ± Log Ä°nceleme

1. Bir workflow Ã§alÄ±ÅŸmasÄ±na tÄ±klayÄ±n
2. **scrape** job'una tÄ±klayÄ±n
3. Her adÄ±mÄ±n detaylÄ± loglarÄ±nÄ± gÃ¶rebilirsiniz

### Artifacts (Yedekler)

Her Ã§alÄ±ÅŸmanÄ±n sonucu Artifacts olarak saklanÄ±r:

1. Workflow Ã§alÄ±ÅŸmasÄ±na gidin
2. SayfanÄ±n altÄ±nda **Artifacts** bÃ¶lÃ¼mÃ¼nÃ¼ bulun
3. `kanun-teklifleri-data-XXX` dosyasÄ±nÄ± indirin

## ğŸ“ˆ Veri EriÅŸimi

### JSON DosyasÄ±

Ã‡ekilen veriler `scraper/data/kanun_teklifleri_sorgu.json` dosyasÄ±nda tutulur ve her Ã§alÄ±ÅŸmada gÃ¼ncellenir.

### Raw Data EriÅŸimi

```bash
# GitHub Ã¼zerinden direkt eriÅŸim
https://raw.githubusercontent.com/KULLANICI_ADI/tbmm-scraper/master/scraper/data/kanun_teklifleri_sorgu.json
```

### API Benzeri KullanÄ±m

GitHub Pages ile birlikte kullanarak basit bir API oluÅŸturabilirsiniz:

1. Repository Settings > Pages > Source: `master branch`
2. Veriye eriÅŸim:
```
https://KULLANICI_ADI.github.io/tbmm-scraper/scraper/data/kanun_teklifleri_sorgu.json
```

## ğŸ”” Bildirimler

### E-posta Bildirimleri

GitHub otomatik olarak baÅŸarÄ±sÄ±z workflow'lar iÃ§in e-posta gÃ¶nderir.

### Slack/Discord Entegrasyonu

Workflow'a notification step'i ekleyebilirsiniz:

```yaml
- name: ğŸ“¬ Slack bildirimi
  uses: 8398a7/action-slack@v3
  with:
    status: ${{ job.status }}
    webhook_url: ${{ secrets.SLACK_WEBHOOK }}
  if: always()
```

## ğŸ› Sorun Giderme

### Workflow Ã‡alÄ±ÅŸmÄ±yor

**Neden:** Repository fork edilmiÅŸse, workflow'lar varsayÄ±lan olarak devre dÄ±ÅŸÄ±dÄ±r.

**Ã‡Ã¶zÃ¼m:**
1. Actions sekmesine gidin
2. "I understand my workflows, go ahead and enable them" butonuna tÄ±klayÄ±n

### Commit Ä°zni HatasÄ±

**Hata:** `permission denied` veya `403 error`

**Ã‡Ã¶zÃ¼m:**
1. Settings > Actions > General
2. Workflow permissions kÄ±smÄ±nda "Read and write permissions" seÃ§in

### ChromeDriver HatasÄ±

**Hata:** `chromedriver not found` veya `session not created`

**Ã‡Ã¶zÃ¼m:** Workflow'daki Chrome kurulum step'i zaten bu sorunu Ã§Ã¶zÃ¼yor. EÄŸer hala sorun varsa, workflow dosyasÄ±nda Chrome versiyonunu gÃ¼ncelleyin.

### Bot KorumasÄ±

**Hata:** Sayfa yÃ¼klenmiyor veya bot korumasÄ± devrede

**Ã‡Ã¶zÃ¼m:**
1. `REQUEST_DELAY` deÄŸerini artÄ±rÄ±n (Ã¶rn: 5 saniye)
2. `time.sleep()` sÃ¼relerini uzatÄ±n
3. User-agent'i gÃ¼ncelleyin

### Veri Ã‡ekilemiyor

**Debug iÃ§in:**

1. Workflow'a debug step ekleyin:

```yaml
- name: ğŸ› Debug - Sayfa iÃ§eriÄŸini kaydet
  if: failure()
  run: |
    if [ -f scraper/debug_page.html ]; then
      echo "Debug page oluÅŸturuldu"
    fi
```

2. Local'de test edin:

```bash
cd scraper
python kanun_teklifleri_scraper.py
```

## ğŸ“ Ä°leri Seviye KullanÄ±m

### Ã‡oklu Sorgu

FarklÄ± parametrelerle birden fazla scraping yapmak iÃ§in:

1. AyrÄ± workflow dosyalarÄ± oluÅŸturun veya
2. Script'i parametrize edin:

```python
import sys

if len(sys.argv) > 1:
    query_type = sys.argv[1]
    if query_type == "kanunlasmis":
        fill_search_form(durum="KANUNLAÅTI")
    elif query_type == "islemde":
        fill_search_form(durum="Ä°ÅLEMDE")
```

Workflow'da:
```yaml
- name: Sorgu 1
  run: python kanun_teklifleri_scraper.py kanunlasmis

- name: Sorgu 2
  run: python kanun_teklifleri_scraper.py islemde
```

### Veri Analizi

Workflow'a analiz adÄ±mÄ± ekleyin:

```yaml
- name: ğŸ“Š Veri analizi
  run: |
    python -c "
    import json
    with open('scraper/data/kanun_teklifleri_sorgu.json', 'r') as f:
        data = json.load(f)
    print(f'Toplam: {len(data)} teklif')
    # Daha fazla analiz...
    "
```

### CSV Export

JSON'u CSV'ye Ã§evirmek iÃ§in:

```yaml
- name: ğŸ“„ CSV'ye Ã§evir
  run: |
    python -c "
    import json, csv
    with open('scraper/data/kanun_teklifleri_sorgu.json', 'r') as f:
        data = json.load(f)
    with open('scraper/data/kanun_teklifleri.csv', 'w', encoding='utf-8') as f:
        if data:
            writer = csv.DictWriter(f, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
    "
```

## ğŸ”’ GÃ¼venlik

- âŒ Workflow dosyasÄ±na API key veya ÅŸifre yazmayÄ±n
- âœ… Hassas bilgiler iÃ§in GitHub Secrets kullanÄ±n
- âœ… Repository private ise, veriler de private kalÄ±r
- âœ… Public repository iÃ§in veri gizliliÄŸine dikkat edin

## ğŸ“š Kaynaklar

- [GitHub Actions DokÃ¼mantasyonu](https://docs.github.com/en/actions)
- [Selenium DokÃ¼mantasyonu](https://www.selenium.dev/documentation/)
- [Cron Expression Generator](https://crontab.guru/)
- [TBMM Web Sitesi](https://www.tbmm.gov.tr)

## ğŸ¤ KatkÄ±da Bulunma

Sorun veya Ã¶neri iÃ§in:
1. Issue aÃ§Ä±n
2. Pull request gÃ¶nderin
3. DokÃ¼mantasyonu geliÅŸtirin

## ğŸ“ Destek

SorunlarÄ±nÄ±z iÃ§in:
- GitHub Issues
- [TBMM API DokÃ¼mantasyonu](https://www.tbmm.gov.tr)

---

**Not:** Bu sistem eÄŸitim ve araÅŸtÄ±rma amaÃ§lÄ±dÄ±r. TBMM web sitesinin kullanÄ±m ÅŸartlarÄ±na uygun kullanÄ±n ve sunucuya gereksiz yÃ¼k bindirmekten kaÃ§Ä±nÄ±n.

