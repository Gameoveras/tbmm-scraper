#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
TBMM Kanun Teklifleri Scraper
Bu script TBMM web sitesinden kanun tekliflerini Ã§eker ve JSON formatÄ±nda kaydeder.
"""

import os
import re
import json
import time
import logging
from typing import List, Dict, Optional
from urllib.parse import urljoin

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# Logging yapÄ±landÄ±rmasÄ±
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Sabitler
BASE_URL = "https://www.tbmm.gov.tr"
LIST_URL = f"{BASE_URL}/Yasama/KanunTeklifi"
DATA_DIR = "data"
OUTPUT_FILE = f"{DATA_DIR}/proposals.json"
REQUEST_DELAY = 2  # Saniye cinsinden bekleme sÃ¼resi
MAX_RETRIES = 3
TIMEOUT = 30

# Global WebDriver instance
driver = None


def create_data_directory():
    """Veri dizinini oluÅŸturur"""
    os.makedirs(DATA_DIR, exist_ok=True)
    logger.info(f"âœ… Veri dizini hazÄ±r: {DATA_DIR}")


def init_driver():
    """Selenium WebDriver'Ä± baÅŸlatÄ±r"""
    global driver
    
    if driver is not None:
        return driver
    
    logger.info("ğŸš€ Selenium WebDriver baÅŸlatÄ±lÄ±yor...")
    
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # TarayÄ±cÄ± gÃ¶sterilmez
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36')
    
    try:
        driver = webdriver.Chrome(options=chrome_options)
        logger.info("âœ… WebDriver baÅŸarÄ±yla baÅŸlatÄ±ldÄ±")
        return driver
    except Exception as e:
        logger.error(f"âŒ WebDriver baÅŸlatÄ±lamadÄ±: {e}")
        raise


def close_driver():
    """Selenium WebDriver'Ä± kapatÄ±r"""
    global driver
    if driver is not None:
        try:
            driver.quit()
            driver = None
            logger.info("âœ… WebDriver kapatÄ±ldÄ±")
        except:
            pass


def fetch_page(url: str, retries: int = MAX_RETRIES) -> Optional[str]:
    """Belirtilen URL'den HTML iÃ§eriÄŸini Ã§eker (Selenium ile)"""
    for attempt in range(1, retries + 1):
        try:
            logger.info(f"ğŸŒ Sayfa Ã§ekiliyor: {url} (Deneme {attempt}/{retries})")
            
            driver = init_driver()
            driver.get(url)
            
            # JavaScript'in yÃ¼klenmesi iÃ§in bekle
            logger.info("â³ JavaScript yÃ¼klenene kadar bekleniyor...")
            time.sleep(5)  # Bot korumasÄ±nÄ±n geÃ§mesi iÃ§in bekle
            
            # SayfanÄ±n body elementinin yÃ¼klenmesini bekle
            WebDriverWait(driver, TIMEOUT).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Sayfa tamamen yÃ¼klendikten sonra HTML'i al
            html = driver.page_source
            
            logger.info(f"âœ… Sayfa baÅŸarÄ±yla Ã§ekildi ({len(html)} karakter)")
            
            # Hala bot korumasÄ± var mÄ± kontrol et
            if len(html) < 10000 or 'challenge' in html.lower():
                logger.warning("âš ï¸ Bot korumasÄ± aktif olabilir, biraz daha bekleniyor...")
                time.sleep(5)
                html = driver.page_source
            
            return html
            
        except Exception as e:
            logger.warning(f"âš ï¸ Hata (Deneme {attempt}/{retries}): {e}")
            if attempt < retries:
                time.sleep(REQUEST_DELAY * 2)
            else:
                logger.error(f"âŒ Sayfa Ã§ekilemedi: {url}")
                return None
    return None


def extract_esas_no(text: str) -> str:
    """Metin iÃ§inden Esas No'yu Ã§Ä±karÄ±r"""
    # Ã–rnek: "Esas No: 2/1234" veya "(2/1234)"
    pattern = r'(?:Esas\s*No[:\s]+)?(\d+/\d+)'
    match = re.search(pattern, text, re.IGNORECASE)
    return match.group(1) if match else ''


def extract_donem_yasama(text: str) -> str:
    """Metin iÃ§inden DÃ¶nem/Yasama YÄ±lÄ± bilgisini Ã§Ä±karÄ±r"""
    # Ã–rnek: "28. DÃ¶nem 2. Yasama YÄ±lÄ±"
    pattern = r'(\d+)\.\s*DÃ¶nem\s+(\d+)\.\s*Yasama\s+YÄ±lÄ±'
    match = re.search(pattern, text, re.IGNORECASE)
    if match:
        return f"{match.group(1)}/{match.group(2)}"
    return ''


def scrape_proposal_list() -> List[Dict[str, str]]:
    """Ana liste sayfasÄ±ndan teklif linklerini Ã§eker"""
    html = fetch_page(LIST_URL)
    if not html:
        return []
    
    soup = BeautifulSoup(html, 'lxml')
    proposals_list = []
    
    # TBMM sitesinin farklÄ± olasÄ± yapÄ±larÄ±nÄ± dene
    # Ã–nce spesifik selector'larÄ± dene
    links = []
    
    # SeÃ§enek 1: Liste sayfasÄ±ndaki table veya ul iÃ§indeki linkler
    content_area = soup.select_one('.icerikMetni, .liste, .kanunListesi, #icerik, main')
    if content_area:
        links = content_area.find_all('a', href=True)
        logger.info(f"ğŸ” Ä°Ã§erik alanÄ±nda {len(links)} link bulundu")
    
    # SeÃ§enek 2: TÃ¼m sayfada kanun teklifi linklerini ara
    if not links:
        all_links = soup.find_all('a', href=True)
        # Detay sayfalarÄ±na giden linkleri filtrele
        links = [
            link for link in all_links 
            if any(keyword in str(link.get('href', '')).lower() 
                   for keyword in ['kanunteklifi', 'kanun_teklifi', 'teklif', '/yasa', '/kt'])
        ]
        logger.info(f"ğŸ” TÃ¼m sayfada {len(links)} potansiyel teklif linki bulundu")
    
    # SeÃ§enek 3: EÄŸer hiÃ§ link bulamadÄ±ysak, table row'larÄ± kontrol et
    if not links:
        table_rows = soup.select('table tr td a')
        links = [link for link in table_rows if link.get('href')]
        logger.info(f"ğŸ” Tablolarda {len(links)} link bulundu")
    
    logger.info(f"ğŸ“‹ Toplam {len(links)} adet link iÅŸlenecek")
    
    seen_urls = set()
    for link in links:
        href = link.get('href', '')
        if not href or href in seen_urls or href == '#':
            continue
        
        # JavaScript veya anchor linklerini atla
        if href.startswith('javascript:') or href.startswith('#'):
            continue
        
        full_url = urljoin(BASE_URL, href)
        title = link.get_text(strip=True)
        
        # BaÅŸlÄ±k yoksa veya Ã§ok kÄ±saysa atla
        if not title or len(title) < 10:
            continue
        
        # AynÄ± URL'yi bir kez ekle
        if full_url not in seen_urls:
            proposals_list.append({
                'baslik': title,
                'link': full_url
            })
            seen_urls.add(full_url)
            logger.debug(f"  âœ“ Eklendi: {title[:50]}...")
    
    logger.info(f"âœ… {len(proposals_list)} benzersiz teklif belirlendi")
    
    # Debug iÃ§in: EÄŸer hiÃ§ teklif bulunamadÄ±ysa, sayfanÄ±n bir kÄ±smÄ±nÄ± logla
    if not proposals_list:
        logger.error("âŒ HiÃ§ teklif bulunamadÄ±! Sayfa yapÄ±sÄ±:")
        logger.error(f"Sayfa baÅŸlÄ±ÄŸÄ±: {soup.title.string if soup.title else 'Yok'}")
        logger.error(f"Ä°Ã§erik uzunluÄŸu: {len(html)} karakter")
        # Ä°lk 500 karakteri logla
        logger.error(f"Sayfa Ã¶nizleme: {html[:500]}")
    
    return proposals_list


def scrape_proposal_detail(proposal: Dict[str, str]) -> Dict[str, str]:
    """Bir teklifin detay sayfasÄ±nÄ± Ã§eker ve iÃ§eriÄŸi parse eder"""
    url = proposal['link']
    logger.info(f"ğŸ“„ Detay Ã§ekiliyor: {proposal['baslik'][:50]}...")
    
    html = fetch_page(url)
    if not html:
        logger.warning(f"âš ï¸ Detay sayfasÄ± Ã§ekilemedi, atlanÄ±yor: {url}")
        return proposal
    
    soup = BeautifulSoup(html, 'lxml')
    
    # Ä°Ã§erik alanÄ±nÄ± bul - Birden fazla selector dene
    content_div = None
    selectors = [
        '#icerik',           # Genel iÃ§erik id'si
        '.icerik',           # Genel iÃ§erik class'Ä±
        '.icerikMetni',      # Ä°Ã§erik metni class'Ä±
        '.kanunMetni',       # Kanun metni Ã¶zel class'Ä±
        '.teklif-metni',     # Teklif metni
        'main',              # HTML5 main elementi
        'article',           # HTML5 article elementi
        '.content',          # Genel content class'Ä±
        '#content',          # Genel content id'si
    ]
    
    for selector in selectors:
        content_div = soup.select_one(selector)
        if content_div:
            logger.debug(f"  Ä°Ã§erik bulundu: {selector}")
            break
    
    # EÄŸer hiÃ§bir selector Ã§alÄ±ÅŸmadÄ±ysa, body'yi kullan
    if not content_div:
        content_div = soup.find('body')
        if content_div:
            logger.warning(f"âš ï¸ Ã–zel selector bulunamadÄ±, body kullanÄ±lÄ±yor")
    
    if content_div:
        # Script ve style etiketlerini temizle
        for tag in content_div.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
            tag.decompose()
        
        full_text = content_div.get_text(separator='\n', strip=True)
        
        # BoÅŸ veya Ã§ok kÄ±sa ise uyar
        if len(full_text) < 100:
            logger.warning(f"âš ï¸ Ä°Ã§erik Ã§ok kÄ±sa ({len(full_text)} karakter): {url}")
            logger.warning(f"Ä°Ã§erik Ã¶nizleme: {full_text[:200]}")
        
        # Esas No ve DÃ¶nem/Yasama bilgisini Ã§Ä±kar
        esas_no = extract_esas_no(full_text)
        donem_yasama = extract_donem_yasama(full_text)
        
        proposal['metin'] = full_text
        proposal['esasNo'] = esas_no if esas_no else 'UNKNOWN'
        proposal['donemYasamaYili'] = donem_yasama if donem_yasama else 'UNKNOWN'
        
        logger.info(f"âœ… Ä°Ã§erik Ã§ekildi ({len(full_text)} karakter, Esas: {esas_no}, DÃ¶nem: {donem_yasama})")
    else:
        logger.error(f"âŒ HiÃ§bir iÃ§erik alanÄ± bulunamadÄ±: {url}")
        proposal['metin'] = ''
        proposal['esasNo'] = ''
        proposal['donemYasamaYili'] = ''
    
    # Rate limiting iÃ§in bekle
    time.sleep(REQUEST_DELAY)
    
    return proposal


def save_to_json(proposals: List[Dict[str, str]]):
    """Teklifleri JSON dosyasÄ±na kaydeder"""
    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(proposals, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ Veriler kaydedildi: {OUTPUT_FILE} ({len(proposals)} teklif)")
    except Exception as e:
        logger.error(f"âŒ JSON kaydetme hatasÄ±: {e}")
        raise


def main():
    """Ana scraper fonksiyonu"""
    logger.info("ğŸš€ TBMM Scraper baÅŸlatÄ±ldÄ±")
    
    try:
        # 1. Veri dizinini oluÅŸtur
        create_data_directory()
        
        # 2. Liste sayfasÄ±nÄ± Ã§ek
        proposals = scrape_proposal_list()
        
        if not proposals:
            logger.warning("âš ï¸ HiÃ§ teklif bulunamadÄ±!")
            # BoÅŸ array kaydet
            save_to_json([])
            return
        
        # 3. Her teklifin detayÄ±nÄ± Ã§ek (ilk 20 teklif ile sÄ±nÄ±rlÄ± - test iÃ§in)
        # Ãœretimde bu limiti kaldÄ±rabilir veya artÄ±rabilirsiniz
        MAX_PROPOSALS = int(os.getenv('MAX_PROPOSALS', '20'))
        proposals_to_scrape = proposals[:MAX_PROPOSALS]
        
        logger.info(f"ğŸ” {len(proposals_to_scrape)} teklifin detayÄ± Ã§ekilecek")
        
        detailed_proposals = []
        for i, proposal in enumerate(proposals_to_scrape, 1):
            logger.info(f"ğŸ“Š Ä°lerleme: {i}/{len(proposals_to_scrape)}")
            detailed = scrape_proposal_detail(proposal)
            
            # Sadece geÃ§erli iÃ§eriÄŸe sahip teklifleri kaydet
            if detailed.get('metin'):
                detailed_proposals.append(detailed)
        
        # 4. JSON'a kaydet
        save_to_json(detailed_proposals)
        
        logger.info(f"âœ… Scraping tamamlandÄ±! Toplam: {len(detailed_proposals)} teklif")
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan durduruldu")
    except Exception as e:
        logger.error(f"âŒ Kritik hata: {e}", exc_info=True)
        raise
    finally:
        # Her durumda WebDriver'Ä± kapat
        close_driver()


if __name__ == "__main__":
    main()

