#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
TBMM Kanun Teklifleri Scraper
Bu script TBMM web sitesinden kanun tekliflerini Ã§eker ve JSON formatÄ±nda kaydeder.
"""

import os
import re
import json
import time
import logging
from typing import List, Dict, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# Logging yapÄ±landÄ±rmasÄ±
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Sabitler
BASE_URL = "https://www.tbmm.gov.tr"
LIST_URL = f"{BASE_URL}/Yasama/KanunTeklifi"
DATA_DIR = "data"
OUTPUT_FILE = f"{DATA_DIR}/proposals.json"
REQUEST_DELAY = 2  # Saniye cinsinden bekleme sÃ¼resi
MAX_RETRIES = 3
TIMEOUT = 30

# User-Agent header
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7",
}


def create_data_directory():
    """Veri dizinini oluÅŸturur"""
    os.makedirs(DATA_DIR, exist_ok=True)
    logger.info(f"âœ… Veri dizini hazÄ±r: {DATA_DIR}")


def fetch_page(url: str, retries: int = MAX_RETRIES) -> Optional[str]:
    """Belirtilen URL'den HTML iÃ§eriÄŸini Ã§eker"""
    for attempt in range(1, retries + 1):
        try:
            logger.info(f"ğŸŒ Sayfa Ã§ekiliyor: {url} (Deneme {attempt}/{retries})")
            response = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
            response.raise_for_status()
            response.encoding = 'utf-8'
            logger.info(f"âœ… Sayfa baÅŸarÄ±yla Ã§ekildi ({len(response.text)} karakter)")
            return response.text
        except requests.RequestException as e:
            logger.warning(f"âš ï¸ Hata (Deneme {attempt}/{retries}): {e}")
            if attempt < retries:
                time.sleep(REQUEST_DELAY * 2)
            else:
                logger.error(f"âŒ Sayfa Ã§ekilemedi: {url}")
                return None
    return None


def extract_esas_no(text: str) -> str:
    """Metin iÃ§inden Esas No'yu Ã§Ä±karÄ±r"""
    # Ã–rnek: "Esas No: 2/1234" veya "(2/1234)"
    pattern = r'(?:Esas\s*No[:\s]+)?(\d+/\d+)'
    match = re.search(pattern, text, re.IGNORECASE)
    return match.group(1) if match else ''


def extract_donem_yasama(text: str) -> str:
    """Metin iÃ§inden DÃ¶nem/Yasama YÄ±lÄ± bilgisini Ã§Ä±karÄ±r"""
    # Ã–rnek: "28. DÃ¶nem 2. Yasama YÄ±lÄ±"
    pattern = r'(\d+)\.\s*DÃ¶nem\s+(\d+)\.\s*Yasama\s+YÄ±lÄ±'
    match = re.search(pattern, text, re.IGNORECASE)
    if match:
        return f"{match.group(1)}/{match.group(2)}"
    return ''


def scrape_proposal_list() -> List[Dict[str, str]]:
    """Ana liste sayfasÄ±ndan teklif linklerini Ã§eker"""
    html = fetch_page(LIST_URL)
    if not html:
        return []
    
    soup = BeautifulSoup(html, 'lxml')
    proposals_list = []
    
    # TBMM sitesinin farklÄ± olasÄ± yapÄ±larÄ±nÄ± dene
    # Ã–nce spesifik selector'larÄ± dene
    links = []
    
    # SeÃ§enek 1: Liste sayfasÄ±ndaki table veya ul iÃ§indeki linkler
    content_area = soup.select_one('.icerikMetni, .liste, .kanunListesi, #icerik, main')
    if content_area:
        links = content_area.find_all('a', href=True)
        logger.info(f"ğŸ” Ä°Ã§erik alanÄ±nda {len(links)} link bulundu")
    
    # SeÃ§enek 2: TÃ¼m sayfada kanun teklifi linklerini ara
    if not links:
        all_links = soup.find_all('a', href=True)
        # Detay sayfalarÄ±na giden linkleri filtrele
        links = [
            link for link in all_links 
            if any(keyword in str(link.get('href', '')).lower() 
                   for keyword in ['kanunteklifi', 'kanun_teklifi', 'teklif', '/yasa', '/kt'])
        ]
        logger.info(f"ğŸ” TÃ¼m sayfada {len(links)} potansiyel teklif linki bulundu")
    
    # SeÃ§enek 3: EÄŸer hiÃ§ link bulamadÄ±ysak, table row'larÄ± kontrol et
    if not links:
        table_rows = soup.select('table tr td a')
        links = [link for link in table_rows if link.get('href')]
        logger.info(f"ğŸ” Tablolarda {len(links)} link bulundu")
    
    logger.info(f"ğŸ“‹ Toplam {len(links)} adet link iÅŸlenecek")
    
    seen_urls = set()
    for link in links:
        href = link.get('href', '')
        if not href or href in seen_urls or href == '#':
            continue
        
        # JavaScript veya anchor linklerini atla
        if href.startswith('javascript:') or href.startswith('#'):
            continue
        
        full_url = urljoin(BASE_URL, href)
        title = link.get_text(strip=True)
        
        # BaÅŸlÄ±k yoksa veya Ã§ok kÄ±saysa atla
        if not title or len(title) < 10:
            continue
        
        # AynÄ± URL'yi bir kez ekle
        if full_url not in seen_urls:
            proposals_list.append({
                'baslik': title,
                'link': full_url
            })
            seen_urls.add(full_url)
            logger.debug(f"  âœ“ Eklendi: {title[:50]}...")
    
    logger.info(f"âœ… {len(proposals_list)} benzersiz teklif belirlendi")
    
    # Debug iÃ§in: EÄŸer hiÃ§ teklif bulunamadÄ±ysa, sayfanÄ±n bir kÄ±smÄ±nÄ± logla
    if not proposals_list:
        logger.error("âŒ HiÃ§ teklif bulunamadÄ±! Sayfa yapÄ±sÄ±:")
        logger.error(f"Sayfa baÅŸlÄ±ÄŸÄ±: {soup.title.string if soup.title else 'Yok'}")
        logger.error(f"Ä°Ã§erik uzunluÄŸu: {len(html)} karakter")
        # Ä°lk 500 karakteri logla
        logger.error(f"Sayfa Ã¶nizleme: {html[:500]}")
    
    return proposals_list


def scrape_proposal_detail(proposal: Dict[str, str]) -> Dict[str, str]:
    """Bir teklifin detay sayfasÄ±nÄ± Ã§eker ve iÃ§eriÄŸi parse eder"""
    url = proposal['link']
    logger.info(f"ğŸ“„ Detay Ã§ekiliyor: {proposal['baslik'][:50]}...")
    
    html = fetch_page(url)
    if not html:
        logger.warning(f"âš ï¸ Detay sayfasÄ± Ã§ekilemedi, atlanÄ±yor: {url}")
        return proposal
    
    soup = BeautifulSoup(html, 'lxml')
    
    # Ä°Ã§erik alanÄ±nÄ± bul - Birden fazla selector dene
    content_div = None
    selectors = [
        '#icerik',           # Genel iÃ§erik id'si
        '.icerik',           # Genel iÃ§erik class'Ä±
        '.icerikMetni',      # Ä°Ã§erik metni class'Ä±
        '.kanunMetni',       # Kanun metni Ã¶zel class'Ä±
        '.teklif-metni',     # Teklif metni
        'main',              # HTML5 main elementi
        'article',           # HTML5 article elementi
        '.content',          # Genel content class'Ä±
        '#content',          # Genel content id'si
    ]
    
    for selector in selectors:
        content_div = soup.select_one(selector)
        if content_div:
            logger.debug(f"  Ä°Ã§erik bulundu: {selector}")
            break
    
    # EÄŸer hiÃ§bir selector Ã§alÄ±ÅŸmadÄ±ysa, body'yi kullan
    if not content_div:
        content_div = soup.find('body')
        if content_div:
            logger.warning(f"âš ï¸ Ã–zel selector bulunamadÄ±, body kullanÄ±lÄ±yor")
    
    if content_div:
        # Script ve style etiketlerini temizle
        for tag in content_div.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
            tag.decompose()
        
        full_text = content_div.get_text(separator='\n', strip=True)
        
        # BoÅŸ veya Ã§ok kÄ±sa ise uyar
        if len(full_text) < 100:
            logger.warning(f"âš ï¸ Ä°Ã§erik Ã§ok kÄ±sa ({len(full_text)} karakter): {url}")
            logger.warning(f"Ä°Ã§erik Ã¶nizleme: {full_text[:200]}")
        
        # Esas No ve DÃ¶nem/Yasama bilgisini Ã§Ä±kar
        esas_no = extract_esas_no(full_text)
        donem_yasama = extract_donem_yasama(full_text)
        
        proposal['metin'] = full_text
        proposal['esasNo'] = esas_no if esas_no else 'UNKNOWN'
        proposal['donemYasamaYili'] = donem_yasama if donem_yasama else 'UNKNOWN'
        
        logger.info(f"âœ… Ä°Ã§erik Ã§ekildi ({len(full_text)} karakter, Esas: {esas_no}, DÃ¶nem: {donem_yasama})")
    else:
        logger.error(f"âŒ HiÃ§bir iÃ§erik alanÄ± bulunamadÄ±: {url}")
        proposal['metin'] = ''
        proposal['esasNo'] = ''
        proposal['donemYasamaYili'] = ''
    
    # Rate limiting iÃ§in bekle
    time.sleep(REQUEST_DELAY)
    
    return proposal


def save_to_json(proposals: List[Dict[str, str]]):
    """Teklifleri JSON dosyasÄ±na kaydeder"""
    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(proposals, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ Veriler kaydedildi: {OUTPUT_FILE} ({len(proposals)} teklif)")
    except Exception as e:
        logger.error(f"âŒ JSON kaydetme hatasÄ±: {e}")
        raise


def main():
    """Ana scraper fonksiyonu"""
    logger.info("ğŸš€ TBMM Scraper baÅŸlatÄ±ldÄ±")
    
    try:
        # 1. Veri dizinini oluÅŸtur
        create_data_directory()
        
        # 2. Liste sayfasÄ±nÄ± Ã§ek
        proposals = scrape_proposal_list()
        
        if not proposals:
            logger.warning("âš ï¸ HiÃ§ teklif bulunamadÄ±!")
            # BoÅŸ array kaydet
            save_to_json([])
            return
        
        # 3. Her teklifin detayÄ±nÄ± Ã§ek (ilk 20 teklif ile sÄ±nÄ±rlÄ± - test iÃ§in)
        # Ãœretimde bu limiti kaldÄ±rabilir veya artÄ±rabilirsiniz
        MAX_PROPOSALS = int(os.getenv('MAX_PROPOSALS', '20'))
        proposals_to_scrape = proposals[:MAX_PROPOSALS]
        
        logger.info(f"ğŸ” {len(proposals_to_scrape)} teklifin detayÄ± Ã§ekilecek")
        
        detailed_proposals = []
        for i, proposal in enumerate(proposals_to_scrape, 1):
            logger.info(f"ğŸ“Š Ä°lerleme: {i}/{len(proposals_to_scrape)}")
            detailed = scrape_proposal_detail(proposal)
            
            # Sadece geÃ§erli iÃ§eriÄŸe sahip teklifleri kaydet
            if detailed.get('metin'):
                detailed_proposals.append(detailed)
        
        # 4. JSON'a kaydet
        save_to_json(detailed_proposals)
        
        logger.info(f"âœ… Scraping tamamlandÄ±! Toplam: {len(detailed_proposals)} teklif")
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan durduruldu")
    except Exception as e:
        logger.error(f"âŒ Kritik hata: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()

